python note book
#PYSPARK_DRIVER_PYTHON=ipython /bin/pyspark --packages com.databricks:spark-csv_2.10:1.3.0
# directory change to bin
$/bin/pyspark --packages com.databricks:spark-csv_2.10:1.3.0
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
df = sqlContext.load(source="com.databricks.spark.csv", path = '/home/ankit/ML/sample_submission.csv')
df.show()


#by default all columns in csv file read as String columns so first need to convert them in specified format
df1 = df.select('activity_id', df.outcome.cast('float').alias('price'))



#We can create a table from the dataframe if we have good knowladge in sql
df.registerTempTable('lol')

#we can apply some sql statement on it
sqlContext.sql("select outcome,count(*) from lol group by outcome").show()


# convert data frame date column from string to date time and than take the minimum as well as maximum date 
df1 = df.select('activity_id', df.date.cast('date'))
>>> df1.agg({"date": "min"}).collect()
[Row(min(date)=datetime.date(2022, 7, 17))]                                     
>>> df1.agg({"date": "max"}).collect()
[Row(max(date)=datetime.date(2023, 8, 31))]    





