

# directory change to bin
$/bin/pyspark --packages com.databricks:spark-csv_2.10:1.3.0
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
df = sqlContext.load(source="com.databricks.spark.csv", path = '/home/ankit/ML/sample_submission.csv')
df.show()


#by default all columns in csv file read as String columns so first need to convert them in specified format
df1 = df.select('activity_id', df.outcome.cast('float').alias('price'))



#We can create a table from the dataframe if we have good knowladge in sql
df.registerTempTable('lol')

#we can apply some sql statement on it
sqlContext.sql("select outcome,count(*) from lol group by outcome").show()
